{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Vision Transformer**"
      ],
      "metadata": {
        "id": "uUZaViffzdCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's the idea behind this?\n",
        "A image can be split into patches, and then we can feed the patches into a multi-head self attension to treat the sequence of patches like sequence of words."
      ],
      "metadata": {
        "id": "7WOsqJd7zhzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Figure 1\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "\n",
        "* **Embedding** = learnable representation(start with random numbers and improve over time)\n",
        "\n",
        "\n",
        "### Four equations\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png)\n",
        "\n",
        "\n",
        "\n",
        "### Table 1\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-table-1.png)"
      ],
      "metadata": {
        "id": "F85qVN4eRRuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are all piplines and formulas that we need. We will implement the ViT-Base model"
      ],
      "metadata": {
        "id": "vwePzuGcRXUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we need to know what's our input, and what's our output after each block. The input is a image, so in order to represent it using matrix, the matrix will have shape (batch, 3, 224, 224), where 3 are RGB color channels and (224,224) are the default reshape resolution for the ViT-Base"
      ],
      "metadata": {
        "id": "ZSEpz3bjRn2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first split a input image into patches, when doing that, you find out the plot is kind of misleading. We do not actually split one image into 9 different images as shown in the pipline, or to say, it's just part of the process."
      ],
      "metadata": {
        "id": "YDcBX4pZSMDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can simply use a conv2d layer to split one image into patches, since the patch size is 16 for ViT base, and the image resolution is 224, so we will have 224 * 224/(16 * 16) = 14 * 14 patches.\n",
        "\n",
        "So after the input image (Batch,3,224,224) pass throught the conv2d, the output will be (Batch, 768, 14, 14) where 768 can be derived from 16*16*3. How can we understand this? It's like we store each patch's data into 768 dimension vector (remember each patch should have shape (3,16,16), we store them into a 768 dim vector), then we have (14*14) such vectors.\n",
        "\n",
        "So the next thing is flatten the output's last two dimension (14,14), so that the output can be (Batch, 768, 196)\n",
        "\n",
        "Finnaly, we will permute the output to be (Batch, 196, 768), so that it means we have 196 patches, each has 768 dim vector to represent them."
      ],
      "metadata": {
        "id": "mAsLahQ8cdeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "c55YlB_mU2G8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pachify(nn.Module):\n",
        "  def __init__(self, patch_size = 16):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.conv2d = nn.Conv2d(in_channels=3,\n",
        "                            out_channels=768, #D size from table 1 #number of filters\n",
        "                            kernel_size=patch_size,\n",
        "                            stride=patch_size,\n",
        "                            padding=0)\n",
        "  def forward(self, x):\n",
        "    x = self.conv2d(x)\n",
        "    x = x.flatten(start_dim=2)\n",
        "    x = x.transpose(1,2)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sL1cIHt5RP16"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn(1,3,224,224)\n",
        "pachify = Pachify()\n",
        "y = pachify(image)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p5QpVSqUtyl",
        "outputId": "7dfd68c5-f95e-42b1-c04f-a4a89bb1534b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing is to add Positonal information to the 196 patches and prepend the class token to the input. unlike the transformer, which uses a function to add fix positional embedding to the input, this paper states that the positonal embedding are just learnable parameters."
      ],
      "metadata": {
        "id": "M-ykDyl1ea4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Add_Positional_Embedding_and_Class_Token(pachified_input):\n",
        "  batch_size = pachified_input.shape[0]\n",
        "  dimension = pachified_input.shape[-1] #768\n",
        "  num_patches = pachified_input.shape[-2] #196\n",
        "  class_token = nn.Parameter(torch.randn(batch_size,1,dimension),requires_grad=True) # learnable\n",
        "  pachified_input_with_class_token = torch.cat((class_token,pachified_input),dim=1)\n",
        "  position_embedding = nn.Parameter(torch.randn(1,num_patches+1,dimension),requires_grad=True) # learnable\n",
        "  return pachified_input_with_class_token + position_embedding"
      ],
      "metadata": {
        "id": "IFXh8sNyVmMR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the plot, before adding positional embedding and class token, there's a linear projection layer."
      ],
      "metadata": {
        "id": "Ij9F2Vt6h_Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedded_Patches(nn.Module):\n",
        "  def __init__(self, patch_size = 16):\n",
        "    super().__init__()\n",
        "    self.pachify = Pachify(patch_size)\n",
        "    self.linear_projection = nn.Linear(in_features = 768,out_features = 768)\n",
        "  def forward(self, x):\n",
        "    x = self.pachify(x)\n",
        "    x = self.linear_projection(x)\n",
        "    x = Add_Positional_Embedding_and_Class_Token(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "3LCGIHf2iNZz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn(1,3,224,224)\n",
        "prepared_input = Embedded_Patches()\n",
        "y = prepared_input(image)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0o9FNYgjN5E",
        "outputId": "720eefc1-d8ce-4c1d-d7e9-3e26365e98f4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we prepared our input from a image tensor into pachified tensor with class token and positional embedding. Shape (batch,197,768)\n",
        "\n",
        "we are ready to build the transformer block.\n",
        "Note that in this jupyter notebook, I will not implement the transformer block from scratch, if you want to know more about transformer, go to the \"Tranformer_Implementation.ipynb\" notebook also in this repo."
      ],
      "metadata": {
        "id": "ecU8lGdai2ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Block(nn.Module):\n",
        "  def __init__(self, embed_size, heads = 4, dropout_rate = 0.1, hidden_units = 3072):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.multi_head_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    # MLP block are just two linear layers, with a ReLU activation function\n",
        "    self.MLP = nn.Sequential(\n",
        "        nn.Linear(embed_size, hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_units, embed_size)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm1(x)\n",
        "    attension_output, _ = self.multi_head_attention(x,x,x)\n",
        "    x = x + self.dropout(attension_output)\n",
        "    x = self.norm2(x)\n",
        "    mlp_output = self.MLP(x)\n",
        "    x = x + self.dropout(mlp_output)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sktDZEV0hTW_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, it's time to combine everything together!"
      ],
      "metadata": {
        "id": "Qg0eFFNrm-lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, in the paper, we should stack the same transformer encoder block for 12 times for ViT-Base."
      ],
      "metadata": {
        "id": "9-7-CSdcpVAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, image_size = 224,\n",
        "               patch_size = 16,\n",
        "               num_classes = 1000,\n",
        "               embed_size = 768,\n",
        "               num_heads = 12,\n",
        "               dropout_rate = 0.1,\n",
        "               hidden_units = 3072,\n",
        "               num_transformer_blocks = 12):  ## All hyperparameters are provided by the paper about ViT-Base\n",
        "    super().__init__()\n",
        "    self.image_size = image_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_classes = num_classes\n",
        "    self.embed_size = embed_size\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_transformer_blocks = num_transformer_blocks\n",
        "    self.embedding_patches = Embedded_Patches(patch_size= self.patch_size)\n",
        "\n",
        "    # Stack the transformer_encoder for 12 times\n",
        "    self.transformer_encoder_blocks = nn.ModuleList([\n",
        "        Transformer_Encoder_Block(embed_size=self.embed_size,\n",
        "                                  heads=self.num_heads,\n",
        "                                  dropout_rate=self.dropout_rate,\n",
        "                                  hidden_units=self.hidden_units)\n",
        "        for _ in range(self.num_transformer_blocks)\n",
        "    ])\n",
        "\n",
        "    # The final MLP layer to get which class it is in 1000 classes\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(self.embed_size),\n",
        "        nn.Linear(self.embed_size, self.num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Prepare the input\n",
        "    x = self.embedding_patches(x)\n",
        "\n",
        "    # pass through the transformer encoder for 12 times\n",
        "    for block in self.transformer_encoder_blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    # get the class probs\n",
        "    y = self.classifier(x[:,0,:]) #0th in 197 patches is the class token\n",
        "\n",
        "    probs = torch.softmax(y, dim=1)\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "sndvHJcCmunw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT()\n",
        "x = torch.randn(1,3,224,224)\n",
        "y = vit(x)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjkESxC5oGTT",
        "outputId": "04176088-abd5-4aec-9b78-9fc46533fb6d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the prob of 1000 classes sum to 1\n",
        "y.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524Wbk-kqFuc",
        "outputId": "9418af78-3b0d-44ac-ec09-e2545d79c33c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIVPLjqxqd_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}